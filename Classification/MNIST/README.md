# MNIST Classification Models

This project includes the implementation of 8 different algorithms for recognizing handwritten digits from the MNIST dataset. Below is a table displaying the accuracy of each model sorted from highest to lowest accuracy.

## Model Accuracy Table

| Model Name                | Accuracy |
|---------------------------|----------|
| Support Vector Machine    | 99%      |
| Gradient Boosting         | 98%      |
| K-Nearest Neighbors       | 98%      |
| Multi-Layer Perceptron   | 98%      |
| Random Forest             | 98%      |
| Logistic Regression       | 97%      |
| Naive Bayes              | 84%      |
| Decision Tree             | 81%      |

## Additional Information

- **Support Vector Machine (SVM)**: One of the best algorithms for classification tasks, achieving very high accuracy.
- **Gradient Boosting**: A powerful method for improving predictions using multiple decision trees.
- **K-Nearest Neighbors**: A non-parametric algorithm that works well in pattern recognition.
- **Multi-Layer Perceptron (MLP)**: A neural network with multiple layers that provides high accuracy.
- **Random Forest**: A combination of several decision trees that offers high accuracy.
- **Logistic Regression**: A linear model suitable for classification tasks.
- **Naive Bayes**: A fast and simple algorithm that performs well under certain conditions.
- **Decision Tree**: A simple and interpretable model that performs well on some problems but has lower accuracy.

## How to Use

To run this project, you need to install the required libraries. Then, you can execute the code and observe the results.

### Install Libraries

```bash
pip install numpy pandas scikit-learn matplotlib

